# ==============================================================================
# ðŸ¦… JS SCANNER CONFIGURATION (Hunter-Architect Edition v3.5)
# Optimized for: Linux VPS + uvloop + TaskGroup + Hybrid Discovery
# ==============================================================================

# --- TARGETING & SCOPE ---
# Global timeout for HTTP requests (per URL)
timeout: 15 # Connection timeout in seconds (reduced for fail-fast behavior)
verify_ssl: false # Keep false for bug bounty (expired certs often host dev/staging)
max_file_size: 5242880 # 5 MB max to keep scans fast for bug bounty
max_file_size_mb_download: 5.0

# --- HYBRID DISCOVERY (The Funnel) ---

# 1. SPEED LAYER: Katana (Go Binary)
# Fastest discovery. Covers 80% of surface area.
katana:
  enabled: false
  depth: 2 # shallower for faster runs
  concurrency: 20 # reduced concurrency for stable runs
  rate_limit: 150 # lower RPS conservative default
  timeout: 60 # faster fail-fast for Katana
  # Custom flags appended to default: -silent -jc -kf all
  args: "-field-scope scope" # Ensure it stays in scope (redundant safety)

# 2. HISTORY LAYER: SubJS
# passive discovery of old/orphaned files
subjs:
  enabled: true
  timeout: 60 # Increased to ensure full archive query completion

# 3. INTELLIGENCE LAYER: Playwright (Browsers)
# Deep scan for lazy-loaded code. RAM intensive.
playwright:
  headless: true
  max_concurrent: 1 # Limit browsers on VPS to avoid OOM
  restart_after: 50 # Restart frequently to prevent Chromium memory leaks
  page_timeout: 30000 # 30s timeout for slow sites (geico.com, service-now.com)
  enable_interactions: true # Toggle smart DOM interactions (set false to disable)

# Performance Optimizations
interaction_delay: 0.3 # Ultra-fast interactions - reduced from 0.5s

# --- ANALYSIS & RECURSION ---

# 4. RECURSIVE DISCOVERY (Missing in original config)
# [CRITICAL] Analysis of found JS to find *more* JS (imports, requires)
recursion:
  enabled: true
  max_depth: 2 # Depth 2 is safe. Depth 3+ can cause infinite loops/exponential explosions.
  validate_with_head: true # Check file exists (HEAD request) before downloading. Saves bandwidth.

# 5. AST & EXTRACTION
ast:
  enabled: true
  max_file_size_mb: 100 # Increased for large webpack bundles

code_splitting:
  detect_dynamic_imports: true
  extract_chunk_map: true
  probe_common_locales: ["en", "es", "fr", "de"]

# --- PERFORMANCE TUNING (High-Performance VPS Optimized) ---

# Threading (Global)
# CRITICAL: 'threads' controls ACTUAL download concurrency
# VPS-OPTIMIZED: Reduced from 200 to prevent local resource saturation and timeout queue buildup
threads: 15 # Reduced from 200 - VPS can't handle 200 concurrent connections

# Batch Processing (The heavy lifting)
batch_processing:
  enabled: true
  # Phase 4: CPU Bound Processing
  process_threads: 10 # Reduced from 100 to match VPS capacity
  cleanup_minified: false

# Download tuning - VPS-OPTIMIZED for stability over raw speed
download:
  chunk_size: 200 # Reduced from 1000 - smaller batches prevent memory pressure on VPS
  skip_preflight: true # Skip HEAD requests for maximum speed - trust the URLs are valid

# Session Management (curl_cffi HTTP Client) - MAXIMUM SPEED CONFIG
# Optimized for high-bandwidth VPS with excellent connectivity
session_management:
  pool_size: 20 # Increased from 2 - more concurrent connections for speed
  rotate_after: 500 # Increased from 100 - keep connections alive longer
  download_timeout: 30 # Increased from 8s to 30s - handle slow domains like getsentry.net

# --- SECRET SCANNING ---

trufflehog_path: "" # Auto-detect
trufflehog_timeout: 300
# CPU Bottleneck Warning: TruffleHog is heavy.
trufflehog_max_concurrent: 10 # Reduced from 50 - VPS CPU limits

# --- ALERTING ---

discord_webhook: "https://discord.com/api/webhooks/1450082381934362726/IAgRuWMbRnzpHaAezmqVPJdg05L3yYe9nG48RCV5Io6ob9ekytIqpCYxQDg6z-ljO7M3"
discord_rate_limit: 60 # Don't spam your own webhook (60s between batches)
discord_status_enabled: false # Silence is golden. Only alerts matter.

notification_batching:
  strategy: "verified_immediate"
  batch_size: 25 # Larger batches for high-volume scans
  group_by_domain: true
  min_delay: 0.5 # Faster Discord updates

# --- LOGGING ---
logging:
  level: "INFO"
  file_enabled: true
  # Note: Code now auto-generates 'scan.log' (DEBUG) and 'errors.log' (WARNING)

# --- SYSTEM STABILITY ---

checkpoint:
  enabled: true
  frequency: 20 # Save less often to reduce disk I/O, rely on memory.
  auto_cleanup: true

retry:
  http_requests: 2 # Reduced retries for fail-fast on flaky networks
  backoff_base: 0.5
  jitter: true

# --- BYPASS & EVASION ---

user_agent: "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
user_agents:
  - "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
  - "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
  - "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
  - "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
  - "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
  - "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0"
  - "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15"
  - "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
  - "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0"
  - "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
  - "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0"
  - "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0"

# --- TIMEOUTS ---
timeouts:
  http_request: 15 # Connection timeout (increased for slow networks like nba.com)
  download_timeout: 20 # Large file download timeout (shorter to fail fast on bad connections)
  playwright_page: 30000 # Match playwright.page_timeout (increased to 30s)
  trufflehog: 300

beautification:
  timeout_small: 120
  timeout_medium: 300
  timeout_large: 900
  timeout_xlarge: 1800

minification_detection:
  sample_size: 10000
  threshold_score: 5

bundle_unpacker:
  enabled: false
  min_file_size: 102400
  timeout: 300
  temp_dir: "temp/unpacked"

minimal_storage: false
max_concurrent_domains: 5 # Reduced from 25 - prevent domain-level parallelism overload on VPS

# ========================================
# v4.1 Performance Improvements (NEW)
# ========================================

# Bloom Filter (Optional - requires pybloom-live)
# Install: pip install pybloom-live
# Provides O(1) hash lookups for faster duplicate detection
bloom_filter:
  enabled: true # Auto-enabled if pybloom-live installed
  capacity: 500000 # Large capacity for mass scanning (auto-scales)
  error_rate: 0.001 # 0.1% false positive rate

# Noise Filter Thresholds (Configuration-Driven)
# Adjust these to control vendor library detection sensitivity
noise_filter:
  min_file_size_kb: 100 # Ignore tiny files (1KB -> 100KB for speed)
  max_newlines: 15 # Heuristic for minified code (tuned)

# Secrets Streaming (Automatic)
# Prevents memory exhaustion on large scans
secrets:
  buffer_size: 10 # Flush every N secrets to disk
  streaming_enabled: true # Always enabled (no config needed)

# Semgrep Static Analysis (v4.2 NEW)
# Requires: pip install semgrep && semgrep login
# Purpose: Fast security pattern detection in downloaded JS files
semgrep:
  enabled: true # Set to true after installing and logging in to Semgrep
  timeout: 360 # Maximum scan time in seconds (6 minutes default)
  max_target_bytes: 200000000 # Max file size to scan (200MB)
  jobs: 20 # Parallel jobs for faster scanning
  chunk_size: 50 # Number of files per semgrep subprocess to avoid timeouts/arglist limits
  ruleset: "p/javascript"
  max_files: 100 # Max number of files to scan with semgrep
  binary_path: "" # Optional: explicit path to semgrep binary (leave empty for auto-detect)
  # How long to wait for `semgrep --version` during validation (seconds)
  version_timeout: 30
  # Number of retries for the semgrep version check (helps on slow first-run startup)
  version_check_retries: 2
