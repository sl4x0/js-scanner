# ==============================================================================
# ðŸ¦… JS SCANNER CONFIGURATION (Hunter-Architect Edition v3.5)
# Optimized for: Linux VPS + uvloop + TaskGroup + Hybrid Discovery
# ==============================================================================

# --- TARGETING & SCOPE ---
# Global timeout for HTTP requests (per URL)
timeout: 15 # Connection timeout in seconds (reduced for fail-fast behavior)
verify_ssl: false # Keep false for bug bounty (expired certs often host dev/staging)
max_file_size: 209715200  # 200 MB limit to avoid huge files
max_file_size_mb_download: 200.0

# --- HYBRID DISCOVERY (The Funnel) ---

# 1. SPEED LAYER: Katana (Go Binary)
# Fastest discovery. Covers 80% of surface area.
katana:
  enabled: true
  depth: 3 # Depth 3 is standard for finding deep JS files
  concurrency: 120 # Aggressive concurrency - you have CPU headroom
  rate_limit: 300 # Requests/sec - doubled for max speed
  timeout: 300 # [CRITICAL] Increased to 5 mins. 100s is too short for depth 3.
  # Custom flags appended to default: -silent -jc -kf all
  args: "-field-scope scope" # Ensure it stays in scope (redundant safety)

# 2. HISTORY LAYER: SubJS
# passive discovery of old/orphaned files
subjs:
  enabled: true
  timeout: 60 # Increased to ensure full archive query completion

# 3. INTELLIGENCE LAYER: Playwright (Browsers)
# Deep scan for lazy-loaded code. RAM intensive.
playwright:
  headless: true
  max_concurrent: 8 # Increased - you have RAM headroom (11.7GB available)
  restart_after: 50 # Restart frequently to prevent Chromium memory leaks
  page_timeout: 30000 # 30s timeout for slow sites (geico.com, service-now.com)

# Performance Optimizations
interaction_delay: 0.3 # Ultra-fast interactions - reduced from 0.5s

# --- ANALYSIS & RECURSION ---

# 4. RECURSIVE DISCOVERY (Missing in original config)
# [CRITICAL] Analysis of found JS to find *more* JS (imports, requires)
recursion:
  enabled: true
  max_depth: 2 # Depth 2 is safe. Depth 3+ can cause infinite loops/exponential explosions.
  validate_with_head: true # Check file exists (HEAD request) before downloading. Saves bandwidth.

# 5. AST & EXTRACTION
ast:
  enabled: true
  max_file_size_mb: 100 # Increased for large webpack bundles

code_splitting:
  detect_dynamic_imports: true
  extract_chunk_map: true
  probe_common_locales: ["en", "es", "fr", "de"]

# --- PERFORMANCE TUNING (uvloop Optimized) ---

# Threading (Global)
# CRITICAL: 'threads' controls ACTUAL download concurrency (not 'download_threads')
# With uvloop, you can handle significantly more I/O concurrency.
threads: 100 # Aggressive - your CPU load is <1.0, use it!

# Batch Processing (The heavy lifting)
batch_processing:
  enabled: true
  # Phase 4: CPU Bound Processing
  process_threads: 100 # Matches increased threads setting
  cleanup_minified: false

# Session Management (curl_cffi HTTP Client)
# Fixes "curl: (18) end of response with X bytes missing" errors
session_management:
  pool_size: 3 # Number of concurrent sessions (3-5 optimal)
  rotate_after: 100 # Rotate sessions every N downloads to prevent staleness
  download_timeout: 30 # 30s default download timeout (progressive timeouts used for larger files)

# --- SECRET SCANNING ---

trufflehog_path: "" # Auto-detect
trufflehog_timeout: 300
# CPU Bottleneck Warning: TruffleHog is heavy.
trufflehog_max_concurrent: 8 # Increased - your CPU is idle, use it!

# --- ALERTING ---

discord_webhook: "https://discord.com/api/webhooks/1450082381934362726/IAgRuWMbRnzpHaAezmqVPJdg05L3yYe9nG48RCV5Io6ob9ekytIqpCYxQDg6z-ljO7M3"
discord_rate_limit: 60 # Don't spam your own webhook (60s between batches)
discord_status_enabled: false # Silence is golden. Only alerts matter.

notification_batching:
  strategy: "verified_immediate"
  batch_size: 25 # Larger batches for high-volume scans
  group_by_domain: true
  min_delay: 0.5 # Faster Discord updates

# --- LOGGING ---
logging:
  level: "INFO"
  file_enabled: true
  # Note: Code now auto-generates 'scan.log' (DEBUG) and 'errors.log' (WARNING)

# --- SYSTEM STABILITY ---

checkpoint:
  enabled: true
  frequency: 20 # Save less often to reduce disk I/O, rely on memory.
  auto_cleanup: true

retry:
  http_requests: 3 # Minimum required by code validation. 3 is balanced.
  backoff_base: 0.5
  jitter: true

# --- BYPASS & EVASION ---

user_agent: "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
user_agents:
  - "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
  - "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
  - "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
  - "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
  - "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
  - "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0"
  - "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15"
  - "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
  - "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0"
  - "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
  - "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0"
  - "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0"

# --- TIMEOUTS ---
timeouts:
  http_request: 15 # Connection timeout (increased for slow networks like nba.com)
  download_timeout: 300 # Large file download timeout (overrides http_request for downloads)
  playwright_page: 30000 # Match playwright.page_timeout (increased to 30s)
  trufflehog: 300

beautification:
  timeout_small: 120
  timeout_medium: 300
  timeout_large: 900
  timeout_xlarge: 1800

minification_detection:
  sample_size: 10000
  threshold_score: 5

bundle_unpacker:
  enabled: false
  min_file_size: 102400
  timeout: 300
  temp_dir: "temp/unpacked"

minimal_storage: false
max_concurrent_domains: 25 # Run more targets in parallel - you have resources

# ========================================
# v4.1 Performance Improvements (NEW)
# ========================================

# Bloom Filter (Optional - requires pybloom-live)
# Install: pip install pybloom-live
# Provides O(1) hash lookups for faster duplicate detection
bloom_filter:
  enabled: true # Auto-enabled if pybloom-live installed
  capacity: 500000 # Large capacity for mass scanning (auto-scales)
  error_rate: 0.001 # 0.1% false positive rate

# Noise Filter Thresholds (Configuration-Driven)
# Adjust these to control vendor library detection sensitivity
noise_filter:
  min_file_size_kb: 1 # Ignore tiny files (1KB minimum)
  max_newlines: 10 # Heuristic for minified code (fewer newlines = vendor lib)

# Secrets Streaming (Automatic)
# Prevents memory exhaustion on large scans
secrets:
  buffer_size: 10 # Flush every N secrets to disk
  streaming_enabled: true # Always enabled (no config needed)


# Semgrep Static Analysis (v4.2 NEW)
# Requires: pip install semgrep && semgrep login
# Purpose: Fast security pattern detection in downloaded JS files
semgrep:
  enabled: true  # Set to true after installing and logging in to Semgrep
  timeout: 120  # Maximum scan time in seconds (2 minutes default)
  max_target_bytes: 2000000  # Max file size to scan (2MB)
  jobs: 8  # Parallel jobs for faster scanning
  ruleset: 'p/javascript'
  max_files: 100
  binary_path: ""  # Optional: explicit path to semgrep binary (leave empty for auto-detect)
