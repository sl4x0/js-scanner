# Discord Integration
# ⚠️ SECURITY WARNING: Never commit real webhook URL to Git!
# Get your webhook from: Discord Server > Settings > Integrations > Webhooks
discord_webhook: "YOUR_DISCORD_WEBHOOK_URL_HERE"
discord_rate_limit: 30  # messages per minute
discord_status_enabled: false  # Only send secret alerts, no start/complete notifications

# TruffleHog
# Cross-platform: Use relative path or leave empty to auto-detect
# Windows: searches for trufflehog.exe | Linux: searches for trufflehog
# Priority: 1) Config path below, 2) Project root, 3) System PATH
trufflehog_path: ""  # Leave empty for auto-detection
trufflehog_timeout: 300  # seconds per file
trufflehog_max_concurrent: 5  # Limit concurrent TruffleHog processes (Issue #2)

# Fetching
threads: 25  # Recommended: 10 (2GB RAM), 25 (4GB RAM), 50 (8GB+ RAM)
max_concurrent_domains: 10  # Number of domains to process concurrently
timeout: 30  # seconds
user_agent: "Mozilla/5.0 (BugBounty/Research)"
max_file_size: 10485760  # 10MB max per JS file

# Discovery Mode Options
skip_live: false  # Set to true to skip live site crawling
verbose: false  # Set to true to show all HTTP errors and debug info
no_scope_filter: false  # Set to true to include CDN and third-party JS files

# Timeouts (Issue #8)
timeouts:
  http_request: 30
  playwright_page: 60000
  trufflehog: 300

# Playwright
playwright:
  headless: true
  max_concurrent: 3
  restart_after: 100  # Restart browser after N pages
  page_timeout: 30000  # ms

# SubJS Configuration  
subjs:
  enabled: true  # Enable SubJS for URL discovery
  timeout: 60  # Timeout per domain (seconds)
  
# Recursive Crawling
recursion:
  enabled: true
  max_depth: 3  # Maximum crawl depth

# AST Analysis (v3.0)
ast:
  min_word_length: 4  # Minimum word length for wordlist
  enable_fragment_filtering: true  # Advanced wordlist filtering (v3.0)

# Bundle Unpacker (v3.0)
# Requires: npm install -g webcrack
bundle_unpacker:
  enabled: false  # Set to true to enable bundle unpacking
  min_file_size: 102400  # Only unpack files >100KB
  timeout: 300  # seconds
  temp_dir: "temp/unpacked"
  
# AST Analysis
ast:
  enabled: true
  min_word_length: 4  # minimum word length for wordlist
  
# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file_enabled: true

# Batch Processing Configuration
batch_processing:
  enabled: true  # Enable new batch workflow
  download_threads: 50  # Max concurrent downloads in Phase 2
  process_threads: 50  # Max concurrent processing in Phase 4
  cleanup_minified: true  # Delete minified files after processing to save disk space

# Retry Configuration (recommended for large URL lists)
retry:
  http_requests: 5  # Max retry attempts for HTTP operations (1 initial + 4 retries)
  subprocess_calls: 2  # Max retry attempts for subprocess operations
  backoff_base: 1.0  # Base delay in seconds for exponential backoff
  backoff_multiplier: 2.0  # Multiplier for exponential backoff
  jitter: true  # Add random jitter to prevent thundering herd

# Connection Pool Settings (for large URL lists)
connection_pool:
  max_connections: 100  # Maximum total connections
  max_per_host: 10  # Maximum connections per host
  ttl_dns_cache: 300  # DNS cache TTL in seconds

# Domain-Specific Organization (NEW)
# Results are automatically organized by domain:
#   extracts/<domain>/endpoints.json - Domain-specific endpoints
#   extracts/<domain>/params.txt - Domain-specific parameters
#   secrets/<domain>/secrets.json - Domain-specific secrets
# Legacy flat files are maintained for backward compatibility
